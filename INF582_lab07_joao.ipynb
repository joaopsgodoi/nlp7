{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1PG8lDimm_pW"
      },
      "source": [
        "# Sequence Labelling\n",
        "\n",
        "In this session we will build an HMM model for PoS-tagging and then CRF and neural models for Named Entity Recognition.\n",
        "\n",
        "## Building a simple Hidden Markov Model\n",
        "\n",
        "In this first part of the lab we will build a very simple bigram HMM using probability estimates over the Brown corpus, which is Part-of-Speech tagged.\n",
        "\n",
        "Recall from course 6: probability estimates (with MLE estimation) can be calculated by dividing the number of occurrences of a bigram by the number of occurrences of the first word.\n",
        "\n",
        "First of all, we import the corpus where we will estimate the probabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzScApjznG03",
        "outputId": "dd0e3c81-243b-479e-a144-a0c2b91ebd73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     /users/eleves-b/2021/joao.sedeu-godoi/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter as NgramCounter\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Msr9l5K7m_pa"
      },
      "source": [
        "This corpus is in the form of sequences of sentences, where each sentence is made by a sequence of pairs (word, POS-tag), like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_GAd0oCm_pb",
        "outputId": "3bcaa519-26f4-45ba-e150-ee4fc96012c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.tagged_sents()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AcZN_ZTbm_pc"
      },
      "source": [
        "We recall (from the course) that a Hidden Markov Model is composed by:\n",
        "\n",
        "- A set of $N$ states $Q = \\{q_1, q_2, \\ldots, q_N\\}$\n",
        "- A transition probability matrix $A=a_{11}\\ldots a_{ij} \\ldots a_{NN}$, where each $a_{ij}$ represents the probability of transitioning from state $q_i$ to $q_j$; note that $\\sum_{j=1}^N{a_{ij}} = 1 \\forall i$\n",
        "- A sequence of $T$ observations $O = o_1, o_2, \\ldots o_T$, each one drawn from a vocabulary of size $V=v_1, v_2, \\ldots, v_M$, of size $M$;\n",
        "- A sequence of *observation likelihoods* $B=b_i(o_t)$, also called **emission probabilities**, each expressing the probability of an observation $o_t$ being generated from a state $q_i$;\n",
        "- Finally, an initial probability distribution $\\Pi = \\pi_i, \\pi_2, \\ldots, \\pi_N$ where $\\pi_i$ indicates the probability that the Markov chain will start from state $q_i$. Some states $q_j$ may have $\\pi_j = 0$, meaning that they cannot be initial states. Also, $\\sum_{i=1}^N{\\pi_i}=1$.\n",
        "\n",
        "In our case, the set of states $Q$ is made by the vocabulary of labels (the POS-tags). The vocabulary $V$ corresponds to the word vocabulary (i.e. all the set of different words that appear in our corpus). The observations correspond to the sentences in the corpus.\n",
        "\n",
        "We will now split our corpus in a training and test set:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ujaEoEwm_pc"
      },
      "outputs": [],
      "source": [
        "corpus = brown.tagged_sents()\n",
        "\n",
        "training = corpus[:-10]\n",
        "testing = corpus[-10:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_22blCd_m_pd"
      },
      "source": [
        "**Exercise 1**: Extract $Q$ and $V$ from the Brown corpus and determine their respective size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id6O7__bm_pd",
        "outputId": "0480cda5-6454-4e6d-87c1-10b7d0731316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Q: 472\n",
            "Size of V: 56057\n"
          ]
        }
      ],
      "source": [
        "#insert your solution here\n",
        "\n",
        "# question 1\n",
        "concat = np.concatenate(corpus)\n",
        "Q = list(set(concat[:,1]))\n",
        "V = list(set(concat[:,0]))\n",
        "\n",
        "print(f\"Size of Q: {len(Q)}\")\n",
        "print(f\"Size of V: {len(V)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQaf0N0m_pd"
      },
      "source": [
        "**Exercise 2**: Create the matrices ($A$, $B$ and $\\Pi$) by using the probabilities estimated on the training set; since we are considering bigrams, the probabilities of the transition matrix will be calculated as $\\frac{count(t_{-1}, t)}{count(t_{-1})}$.\n",
        "\n",
        "*Important*: you will need to add smoothing (for instance Lidstone with $k=0.1$) on $B$ otherwise the output will be $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SH8RreuZm_pd"
      },
      "outputs": [],
      "source": [
        "# question 2\n",
        "\n",
        "def assign_indexes(items):\n",
        "    indexes = {}\n",
        "    for i, item in enumerate(items):\n",
        "        indexes[item] = i\n",
        "    return indexes\n",
        "\n",
        "N = len(Q)\n",
        "M = len(V)\n",
        "k = 0.1\n",
        "\n",
        "A = np.zeros((N, N))\n",
        "B = k * np.ones((N, M))\n",
        "pi = np.zeros(N)\n",
        "\n",
        "tag_indexes = assign_indexes(Q)\n",
        "word_indexes = assign_indexes(V)\n",
        "\n",
        "for sent in training:\n",
        "    for i, pair in enumerate(sent):\n",
        "        tag_index = tag_indexes[pair[1]]\n",
        "        word_index = word_indexes[pair[0]]\n",
        "        pi[tag_index] += 1\n",
        "        B[tag_index][word_index] += 1\n",
        "        if i < len(sent)-1:\n",
        "            next_tag_index = tag_indexes[sent[i + 1][1]]\n",
        "            A[tag_index][next_tag_index] += 1\n",
        "\n",
        "A /= np.sum(A, axis=1)[:, np.newaxis]\n",
        "B /= np.sum(B, axis=1)[:, np.newaxis]\n",
        "pi /= np.sum(pi)\n",
        "\n",
        "# end of question 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I0xXjLINm_pd"
      },
      "source": [
        "We have now a model and we can estimate its performance over the test set.\n",
        "\n",
        "To do this, we need the Viterbi algorithm for the decoding. To help you, an implementation of Viterbi is provided:\n",
        "(note: to use this version you need to assign a numeric id to each word and tag, if you haven't already)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s7PDvT2em_pe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "params is a triple (pi, A, B) where\n",
        "pi = initial probability distribution over states\n",
        "A = transition probability matrix\n",
        "B = emission probability matrix\n",
        "\n",
        "observations is the sequence of observations (in our case, the observed words)\n",
        "\n",
        "the function returns the optimal sequence of states and its score\n",
        "\"\"\"\n",
        "def viterbi(params, observations):\n",
        "    pi, A, B = params\n",
        "    M = len(observations)\n",
        "    S = pi.shape[0]\n",
        "\n",
        "    alpha = np.zeros((M, S))\n",
        "    alpha[:,:] = float('-inf') #cases that have not been treated\n",
        "    backpointers = np.zeros((M, S), 'int')\n",
        "\n",
        "    # base case\n",
        "    alpha[0, :] = pi * B[:,observations[0]]\n",
        "\n",
        "    # recursive case\n",
        "    for t in range(1, M):\n",
        "        for s2 in range(S):\n",
        "            for s1 in range(S):\n",
        "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
        "                if score > alpha[t, s2]:\n",
        "                    alpha[t, s2] = score\n",
        "                    backpointers[t, s2] = s1\n",
        "    # now follow backpointers to resolve the state sequence\n",
        "    ss = []\n",
        "    ss.append(np.argmax(alpha[M-1,:]))\n",
        "    for i in range(M-1, 0, -1):\n",
        "        ss.append(backpointers[i, ss[-1]])\n",
        "\n",
        "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
        "\n",
        "#Example:\n",
        "\n",
        "#original sentence: you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
        "#sentence as sequence of word indexes:\n",
        "#word_index=[42350, 44913, 3024, 50638, 15858, 16209, 36949, 31092, 28334, 45518, 22719, 26179, 32651, 52996, 25205, 16840, 36949, 1402, 46003, 10606, 19795, 3739]\n",
        "\n",
        "#predicted, score = viterbi((pi, A, B), word_index)\n",
        "\n",
        "#predicted will be a sequence of tag indexes:\n",
        "#[12, 55, 86, 39, 29, 4, 70, 7, 14, 7, 0, 6, 21, 28, 12, 55, 28, 27, 28, 0, 9, 14, 15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVtsIEPUm_pe",
        "outputId": "43bebd17-59cd-426c-efa3-2c9f7d8c7c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PPSS         PPSS\n",
            "      MD*         MD*\n",
            "      QL         QL\n",
            "      RB         RB\n",
            "      VB         VBD\n",
            "      IN         RP\n",
            "      IN         IN\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      VB         VB\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VB         VB\n",
            "      TO         TO\n",
            "      VB         VB\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NP         .\n",
            "      .         .\n",
            "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
            "   ##TRUE##    ##PRED##\n",
            "      RB         RB\n",
            "      ,         ,\n",
            "      CS         CS\n",
            "      PPSS+BER         PPSS+BER\n",
            "      VBG         VBG\n",
            "      TO         TO\n",
            "      BE         BE\n",
            "      FW-RB         VBN\n",
            "      FW-IN         TO\n",
            "      FW-NN         VB\n",
            "      QL         QL\n",
            "      RB         RB\n",
            "      IN         IN\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      NNS         NP\n",
            "      ,         ,\n",
            "      NP         PPSS\n",
            "      NN         VB\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      NP         VBN-TL\n",
            "      NN         NNS-TL\n",
            "      ,         ,\n",
            "      NN         NN\n",
            "      NN         ''\n",
            "      ,         ,\n",
            "      NN         NP\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      AP         AP\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         ``\n",
            "      PPSS+HV         PPSS+HV\n",
            "      VBN         VBN\n",
            "      ,         ,\n",
            "      PPSS         PPSS\n",
            "      MD*         MD*\n",
            "      VB         VB\n",
            "      TO         TO\n",
            "      BE         BE\n",
            "      VBN         VBN\n",
            "      RB         RB\n",
            "      .         .\n",
            "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VB         VB\n",
            "      JJ         JJ\n",
            "      QLP         QLP\n",
            "      TO         TO\n",
            "      VB         VB\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      BEZ         BEZ\n",
            "      CS         CS\n",
            "      AT         AT\n",
            "      AP         AP\n",
            "      NN         NN\n",
            "      PPSS+BER         PPSS+BER\n",
            "      VBN         VBN\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      ,         ,\n",
            "      VB         VB\n",
            "      PPO         PPO\n",
            "      RP         RP\n",
            "      --         --\n",
            "      PPS+BEZ         PPS+BEZ\n",
            "      PN         PN\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      .         .\n",
            "  The sentence is:  As you can count on me to do the same .\n",
            "   ##TRUE##    ##PRED##\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      MD         MD\n",
            "      VB         VB\n",
            "      IN         IN\n",
            "      PPO         PPO\n",
            "      TO         TO\n",
            "      DO         DO\n",
            "      AT         AT\n",
            "      AP         AP\n",
            "      .         .\n",
            "  The sentence is:  Compassionately yours ,\n",
            "   ##TRUE##    ##PRED##\n",
            "      RB         CS\n",
            "      PP$$         PP$$\n",
            "      ,         ,\n",
            "  The sentence is:  S. J. Perelman\n",
            "   ##TRUE##    ##PRED##\n",
            "      NP         NP\n",
            "      NP         NP\n",
            "      NP         NP\n",
            "  The sentence is:  revulsion in the desert\n",
            "   ##TRUE##    ##PRED##\n",
            "      NN-HL         NN\n",
            "      IN-HL         IN\n",
            "      AT-HL         AT\n",
            "      NN-HL         NN\n",
            "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
            "   ##TRUE##    ##PRED##\n",
            "      AT         AT\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NP-TL         NN\n",
            "      NN         NN\n",
            "      VBD         VBD\n",
            "      VBN         VBN\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VBD         VBD\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      ,         ,\n",
            "      VBG         NP\n",
            "      ,         ,\n",
            "      VBD         VBD\n",
            "      RP         RP\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NN         NNS\n",
            "      IN         IN\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      VBD         NN\n",
            "      IN         IN\n",
            "      NN         NN\n",
            "      .         .\n",
            "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PPS         PPS\n",
            "      BEDZ         BEDZ\n",
            "      AT         AT\n",
            "      VBG         VBG\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      --         --\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NNS         NNS\n",
            "      ,         ,\n",
            "      JJ         NP\n",
            "      ,         ,\n",
            "      WPS         WPS\n",
            "      VBD         VBD\n",
            "      AT         AT\n",
            "      NP         JJ\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      PP$         PP$\n",
            "      JJ-TL         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      WP$         WP$\n",
            "      AP         AP\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN$         NN$\n",
            "      VBG         NN\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      BEDZ         BEDZ\n",
            "      CS         CS\n",
            "      AT         AT\n",
            "      JJR         JJR\n",
            "      NN         NN\n",
            "      BEDZ         BEDZ\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      QL         QL\n",
            "      JJ         JJ\n",
            "      .         .\n",
            "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
            "   ##TRUE##    ##PRED##\n",
            "      IN         IN\n",
            "      WDT         WDT\n",
            "      PPSS         PPSS\n",
            "      BEDZ         BEDZ\n",
            "      JJ         JJ\n",
            "      IN         IN\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      ,         ,\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         QL\n",
            "      NN         JJ\n",
            "      NN         NN\n",
            "      BEDZ         BEDZ\n",
            "      VBG         VBN\n",
            "      .         .\n"
          ]
        }
      ],
      "source": [
        "#Example of results calculation\n",
        "\n",
        "testing_formated = []\n",
        "for sentence in testing:\n",
        "    sent = \"\"\n",
        "    words_index = [] #vector of word indices to be passed to Viterbi\n",
        "    true_label= [] #vector of the true labels from labeled corpus\n",
        "    for word,tag in sentence:\n",
        "        words_index.append(word_indexes[word]) #word_to_index is a dictionary mapping a word to its index\n",
        "        true_label.append(tag)\n",
        "        sent=sent+\" \"+word\n",
        "    testing_formated.append((words_index,true_label,sent))\n",
        "\n",
        "\n",
        "for word_index,labels,sentence in testing_formated:\n",
        "    print(\"  The sentence is:\",sentence)\n",
        "    print(\"   ##TRUE##    ##PRED##\")\n",
        "    predicted, score = viterbi((pi, A, B), word_index) #call the viterbi decoder\n",
        "    for i,true_label in enumerate(labels):\n",
        "        predicted_label = Q[predicted[i]] #Q here is the vector of tags, so that at Q[i] we have the i_th tag in literal form\n",
        "        print(\"      \"+true_label+\"         \"+predicted_label)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xJx93HTUm_pe"
      },
      "source": [
        "**Exercise 3**: calculate Precision, Recall and F-measure for the bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RUi8ggb8m_pf"
      },
      "outputs": [],
      "source": [
        "# question 3\n",
        "\n",
        "yp, yt = [], []\n",
        "for wi, l, sent in testing_formated:\n",
        "    pred, _ = viterbi((pi, A, B), wi)\n",
        "    for i, tl in enumerate(l):\n",
        "        yp.append(Q[pred[i]])\n",
        "        yt.append(tl)\n",
        "\n",
        "# end of question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmJ5ovUvm_pf",
        "outputId": "0be37c3a-61b8-4dc6-ac76-7556d95a4ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ''       0.00      0.00      0.00         0\n",
            "           ,       1.00      1.00      1.00        24\n",
            "          --       1.00      1.00      1.00         2\n",
            "           .       0.88      1.00      0.93         7\n",
            "          AP       1.00      1.00      1.00         4\n",
            "          AT       0.96      1.00      0.98        26\n",
            "       AT-HL       0.00      0.00      0.00         1\n",
            "          BE       1.00      1.00      1.00         2\n",
            "        BEDZ       1.00      1.00      1.00         5\n",
            "         BEZ       1.00      1.00      1.00         1\n",
            "          CC       1.00      1.00      1.00         7\n",
            "          CS       0.88      1.00      0.93         7\n",
            "          DO       1.00      1.00      1.00         1\n",
            "       FW-IN       0.00      0.00      0.00         1\n",
            "       FW-NN       0.00      0.00      0.00         1\n",
            "       FW-RB       0.00      0.00      0.00         1\n",
            "          IN       0.95      0.95      0.95        19\n",
            "       IN-HL       0.00      0.00      0.00         1\n",
            "          JJ       0.77      0.83      0.80        12\n",
            "       JJ-TL       0.00      0.00      0.00         1\n",
            "         JJR       1.00      1.00      1.00         1\n",
            "          MD       1.00      1.00      1.00         1\n",
            "         MD*       1.00      1.00      1.00         2\n",
            "          NN       0.84      0.79      0.82        34\n",
            "         NN$       1.00      1.00      1.00         1\n",
            "       NN-HL       0.00      0.00      0.00         2\n",
            "         NNS       0.80      0.80      0.80         5\n",
            "      NNS-TL       0.00      0.00      0.00         0\n",
            "          NP       0.43      0.43      0.43         7\n",
            "       NP-TL       0.00      0.00      0.00         1\n",
            "          PN       1.00      1.00      1.00         1\n",
            "         PP$       1.00      1.00      1.00         4\n",
            "        PP$$       1.00      1.00      1.00         1\n",
            "         PPO       1.00      1.00      1.00         2\n",
            "         PPS       1.00      1.00      1.00         1\n",
            "     PPS+BEZ       1.00      1.00      1.00         1\n",
            "        PPSS       0.88      1.00      0.93         7\n",
            "    PPSS+BER       1.00      1.00      1.00         2\n",
            "     PPSS+HV       1.00      1.00      1.00         1\n",
            "          QL       0.75      1.00      0.86         3\n",
            "         QLP       1.00      1.00      1.00         1\n",
            "          RB       1.00      0.80      0.89         5\n",
            "          RP       0.67      1.00      0.80         2\n",
            "          TO       0.83      1.00      0.91         5\n",
            "          VB       0.80      0.89      0.84         9\n",
            "         VBD       0.80      0.80      0.80         5\n",
            "         VBG       1.00      0.40      0.57         5\n",
            "         VBN       0.67      1.00      0.80         4\n",
            "      VBN-TL       0.00      0.00      0.00         0\n",
            "         WDT       1.00      1.00      1.00         1\n",
            "         WP$       1.00      1.00      1.00         1\n",
            "         WPS       1.00      1.00      1.00         1\n",
            "          ``       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87       239\n",
            "   macro avg       0.71      0.73      0.72       239\n",
            "weighted avg       0.86      0.87      0.86       239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true=yt, y_pred=yp, zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rIjJzyCbm_pf"
      },
      "source": [
        "**Exercise 4**: modify your HMM to use trigrams instead of bigrams, and re-evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_6Xr6wH3o9K",
        "outputId": "c2be4a89-6dc8-4e43-cb62-f5270f3db29e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1347756/234607826.py:33: RuntimeWarning: invalid value encountered in divide\n",
            "  A /= np.sum(A, axis=1)[:, np.newaxis]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           ,       0.92      0.96      0.94        24\n",
            "          --       1.00      1.00      1.00         2\n",
            "           .       1.00      1.00      1.00         7\n",
            "          AP       1.00      1.00      1.00         4\n",
            "          AT       0.86      0.96      0.91        26\n",
            "       AT-HL       0.00      0.00      0.00         1\n",
            "          BE       1.00      1.00      1.00         2\n",
            "        BEDZ       0.83      1.00      0.91         5\n",
            "         BEZ       1.00      1.00      1.00         1\n",
            "          CC       0.86      0.86      0.86         7\n",
            "          CS       0.78      1.00      0.88         7\n",
            "          DO       1.00      1.00      1.00         1\n",
            "       FW-IN       0.00      0.00      0.00         1\n",
            "       FW-NN       0.00      0.00      0.00         1\n",
            "       FW-RB       0.00      0.00      0.00         1\n",
            "          IN       0.78      0.95      0.86        19\n",
            "       IN-HL       0.00      0.00      0.00         1\n",
            "          JJ       0.71      0.83      0.77        12\n",
            "       JJ-TL       0.00      0.00      0.00         1\n",
            "         JJR       1.00      1.00      1.00         1\n",
            "          MD       1.00      1.00      1.00         1\n",
            "         MD*       1.00      1.00      1.00         2\n",
            "          NN       0.77      0.88      0.82        34\n",
            "         NN$       1.00      1.00      1.00         1\n",
            "       NN-HL       0.00      0.00      0.00         2\n",
            "         NNS       0.57      0.80      0.67         5\n",
            "          NP       1.00      0.29      0.44         7\n",
            "       NP-TL       0.00      0.00      0.00         1\n",
            "          PN       1.00      1.00      1.00         1\n",
            "         PP$       0.80      1.00      0.89         4\n",
            "        PP$$       0.00      0.00      0.00         1\n",
            "         PPO       1.00      1.00      1.00         2\n",
            "         PPS       0.50      1.00      0.67         1\n",
            "     PPS+BEZ       0.00      0.00      0.00         1\n",
            "        PPSS       1.00      1.00      1.00         7\n",
            "    PPSS+BER       1.00      0.50      0.67         2\n",
            "     PPSS+HV       0.00      0.00      0.00         1\n",
            "          QL       1.00      0.67      0.80         3\n",
            "         QLP       1.00      1.00      1.00         1\n",
            "          RB       0.67      0.40      0.50         5\n",
            "          RP       1.00      1.00      1.00         2\n",
            "          TO       0.83      1.00      0.91         5\n",
            "          VB       0.80      0.89      0.84         9\n",
            "         VBD       0.80      0.80      0.80         5\n",
            "         VBG       1.00      0.40      0.57         5\n",
            "         VBN       0.60      0.75      0.67         4\n",
            "         WDT       1.00      1.00      1.00         1\n",
            "         WP$       1.00      1.00      1.00         1\n",
            "         WPS       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.83       239\n",
            "   macro avg       0.68      0.67      0.66       239\n",
            "weighted avg       0.80      0.83      0.80       239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def concatenate_state(state1, state2):\n",
        "    return state1 + ' ' + state2\n",
        "\n",
        "def prepare_data(corpus, training, testing):\n",
        "    corpus_concat = np.concatenate(corpus)\n",
        "    states = [concatenate_state(corpus_concat[i][1], corpus_concat[i+1][1]) for i in range(len(corpus_concat)-1)]\n",
        "    Q = list(set(states))\n",
        "\n",
        "    k = 0.1\n",
        "\n",
        "    N = len(Q)\n",
        "    M = len(V)\n",
        "\n",
        "    A = np.zeros((N, N))\n",
        "    B = k * np.ones((N, M))\n",
        "    pi = np.zeros(N)\n",
        "\n",
        "    q_indexes = {q: i for i, q in enumerate(Q)}\n",
        "    v_indexes = {v: i for i, v in enumerate(V)}\n",
        "\n",
        "    for sentence in training:\n",
        "        for i, s in enumerate(sentence[1:], 1):\n",
        "            s_prev = sentence[i-1]\n",
        "            q = q_indexes[concatenate_state(s_prev[1], s[1])]\n",
        "            v = v_indexes[s[0]]\n",
        "            pi[q] += 1\n",
        "            B[q][v] += 1\n",
        "            if i < len(sentence)-1:\n",
        "                s_next = sentence[i+1]\n",
        "                q_next = Q.index(concatenate_state(s[1], s_next[1]))\n",
        "                A[q][q_next] += 1\n",
        "\n",
        "    A /= np.sum(A, axis=1)[:, np.newaxis]\n",
        "    B /= np.sum(B, axis=1)[:, np.newaxis]\n",
        "    pi /= np.sum(pi)\n",
        "\n",
        "    testing_formatted = []\n",
        "    for sentence in testing:\n",
        "        words_index = []\n",
        "        true_label = []\n",
        "        for word, tag in sentence:\n",
        "            words_index.append(v_indexes[word])\n",
        "            true_label.append(tag)\n",
        "        testing_formatted.append((words_index, true_label))\n",
        "\n",
        "    return pi, A, B, Q, testing_formatted\n",
        "\n",
        "def beam_search(params, observations, k=2):\n",
        "    pi, A, B = params\n",
        "    M = len(observations)\n",
        "    S = pi.shape[0]\n",
        "\n",
        "    alpha = np.zeros((M, S))\n",
        "    alpha[:,:] = float('-inf')\n",
        "    backpointers = np.zeros((M, S), 'int')\n",
        "\n",
        "    alpha[0, :] = pi * B[:, observations[0]]\n",
        "\n",
        "    saved_states = np.argpartition(-alpha[0, :], k)[:k]\n",
        "\n",
        "    for t in range(1, M):\n",
        "        for s2 in range(S):\n",
        "            for s1 in saved_states:\n",
        "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
        "                if score > alpha[t, s2]:\n",
        "                    alpha[t, s2] = score\n",
        "                    backpointers[t, s2] = s1\n",
        "        saved_states = np.argpartition(-alpha[t, :], k)[:k]\n",
        "    ss = []\n",
        "    ss.append(np.argmax(alpha[M-1,:]))\n",
        "    for i in range(M-1, 0, -1):\n",
        "        ss.append(backpointers[i, ss[-1]])\n",
        "\n",
        "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
        "\n",
        "def evaluate_model(testing_formatted, Q):\n",
        "    true_labels, predicted_labels = [], []\n",
        "    for word_index, labels in testing_formatted:\n",
        "        predicted, _ = beam_search((pi, A, B), word_index, 2)\n",
        "        predicted_labels.append([Q[predicted[i]].split(\" \")[1] for i, true_label in enumerate(labels)])\n",
        "        true_labels.append(labels)\n",
        "\n",
        "    print(classification_report(np.concatenate(true_labels), np.concatenate(predicted_labels), zero_division=0))\n",
        "\n",
        "pi, A, B, Q, testing_formatted = prepare_data(corpus, training, testing)\n",
        "evaluate_model(testing_formatted, Q)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY--ariYx28B",
        "outputId": "9f595fb3-7949-4d5c-deab-20bb8a319a2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1347756/1368790750.py:36: RuntimeWarning: invalid value encountered in divide\n",
            "  A /= np.sum(A, axis=1)[:, np.newaxis]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  The sentence is:  you can't very well sidle up to people on the street and ask if they want to buy a hot Bodhisattva .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PPSS         PPSS\n",
            "      MD*         MD*\n",
            "      QL         VB\n",
            "      RB         RB\n",
            "      VB         IN\n",
            "      IN         IN\n",
            "      IN         IN\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      VB         VB\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VB         VB\n",
            "      TO         TO\n",
            "      VB         VB\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NP         NN\n",
            "      .         .\n",
            "  The sentence is:  Additionally , since you're going to be hors de combat pretty soon with sprue , yaws , Delhi boil , the Granville wilt , liver fluke , bilharziasis , and a host of other complications of the hex you've aroused , you mustn't expect to be lionized socially .\n",
            "   ##TRUE##    ##PRED##\n",
            "      RB         NN\n",
            "      ,         ,\n",
            "      CS         CS\n",
            "      PPSS+BER         PPSS+BER\n",
            "      VBG         VBG\n",
            "      TO         TO\n",
            "      BE         BE\n",
            "      FW-RB         VBN\n",
            "      FW-IN         IN\n",
            "      FW-NN         NN\n",
            "      QL         QL\n",
            "      RB         RB\n",
            "      IN         IN\n",
            "      NN         PP$\n",
            "      ,         JJ\n",
            "      NNS         NN\n",
            "      ,         ,\n",
            "      NP         AT\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      NP         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      NN         NN\n",
            "      NN         NNS\n",
            "      ,         ,\n",
            "      NN         NNS\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      AP         AP\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      PPSS+HV         NN\n",
            "      VBN         VBN\n",
            "      ,         ,\n",
            "      PPSS         PPSS\n",
            "      MD*         MD*\n",
            "      VB         VB\n",
            "      TO         TO\n",
            "      BE         BE\n",
            "      VBN         VBN\n",
            "      RB         IN\n",
            "      .         .\n",
            "  The sentence is:  My advice , if you live long enough to continue your vocation , is that the next time you're attracted by the exotic , pass it up -- it's nothing but a headache .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VB         VB\n",
            "      JJ         JJ\n",
            "      QLP         QLP\n",
            "      TO         TO\n",
            "      VB         VB\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      BEZ         BEZ\n",
            "      CS         CS\n",
            "      AT         AT\n",
            "      AP         AP\n",
            "      NN         NN\n",
            "      PPSS+BER         ,\n",
            "      VBN         VBD\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      ,         ,\n",
            "      VB         VB\n",
            "      PPO         PPO\n",
            "      RP         RP\n",
            "      --         --\n",
            "      PPS+BEZ         CC\n",
            "      PN         PN\n",
            "      CC         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      .         .\n",
            "  The sentence is:  As you can count on me to do the same .\n",
            "   ##TRUE##    ##PRED##\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      MD         MD\n",
            "      VB         VB\n",
            "      IN         IN\n",
            "      PPO         PPO\n",
            "      TO         TO\n",
            "      DO         DO\n",
            "      AT         AT\n",
            "      AP         AP\n",
            "      .         .\n",
            "  The sentence is:  Compassionately yours ,\n",
            "   ##TRUE##    ##PRED##\n",
            "      RB         AT\n",
            "      PP$$         NN\n",
            "      ,         ,\n",
            "  The sentence is:  S. J. Perelman\n",
            "   ##TRUE##    ##PRED##\n",
            "      NP         NP\n",
            "      NP         NP\n",
            "      NP         ,\n",
            "  The sentence is:  revulsion in the desert\n",
            "   ##TRUE##    ##PRED##\n",
            "      NN-HL         NN\n",
            "      IN-HL         IN\n",
            "      AT-HL         AT\n",
            "      NN-HL         NN\n",
            "  The sentence is:  the doors of the D train slid shut , and as I dropped into a seat and , exhaling , looked up across the aisle , the whole aviary in my head burst into song .\n",
            "   ##TRUE##    ##PRED##\n",
            "      AT         AT\n",
            "      NNS         NNS\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NP-TL         NN\n",
            "      NN         NN\n",
            "      VBD         VBD\n",
            "      VBN         VBN\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      CS         CS\n",
            "      PPSS         PPSS\n",
            "      VBD         VBD\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      ,         ,\n",
            "      VBG         CS\n",
            "      ,         ,\n",
            "      VBD         VBD\n",
            "      RP         RP\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      PP$         PP$\n",
            "      NN         NN\n",
            "      VBD         VBD\n",
            "      IN         IN\n",
            "      NN         NN\n",
            "      .         .\n",
            "  The sentence is:  She was a living doll and no mistake -- the blue-black bang , the wide cheekbones , olive-flushed , that betrayed the Cherokee strain in her Midwestern lineage , and the mouth whose only fault , in the novelist's carping phrase , was that the lower lip was a trifle too voluptuous .\n",
            "   ##TRUE##    ##PRED##\n",
            "      PPS         PPS\n",
            "      BEDZ         BEDZ\n",
            "      AT         AT\n",
            "      VBG         VBG\n",
            "      NN         NN\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      --         --\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NNS         NNS\n",
            "      ,         ,\n",
            "      JJ         NNS\n",
            "      ,         ,\n",
            "      WPS         CS\n",
            "      VBD         PPS\n",
            "      AT         BEDZ\n",
            "      NP         AT\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      PP$         PP$\n",
            "      JJ-TL         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      CC         CC\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      WP$         WP$\n",
            "      AP         AP\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      NN$         NN$\n",
            "      VBG         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      BEDZ         BEDZ\n",
            "      CS         CS\n",
            "      AT         AT\n",
            "      JJR         JJR\n",
            "      NN         NN\n",
            "      BEDZ         BEDZ\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      QL         QL\n",
            "      JJ         RB\n",
            "      .         .\n",
            "  The sentence is:  From what I was able to gauge in a swift , greedy glance , the figure inside the coral-colored boucle dress was stupefying .\n",
            "   ##TRUE##    ##PRED##\n",
            "      IN         IN\n",
            "      WDT         WDT\n",
            "      PPSS         PPSS\n",
            "      BEDZ         BEDZ\n",
            "      JJ         JJ\n",
            "      IN         TO\n",
            "      NN         VB\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      ,         ,\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      ,         ,\n",
            "      AT         AT\n",
            "      NN         NN\n",
            "      IN         IN\n",
            "      AT         AT\n",
            "      JJ         JJ\n",
            "      NN         NN\n",
            "      NN         NN\n",
            "      BEDZ         BEDZ\n",
            "      VBG         VBN\n",
            "      .         .\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           ,       0.92      0.96      0.94        24\n",
            "          --       1.00      1.00      1.00         2\n",
            "           .       1.00      1.00      1.00         7\n",
            "          AP       1.00      1.00      1.00         4\n",
            "          AT       0.86      0.96      0.91        26\n",
            "       AT-HL       0.00      0.00      0.00         1\n",
            "          BE       1.00      1.00      1.00         2\n",
            "        BEDZ       0.83      1.00      0.91         5\n",
            "         BEZ       1.00      1.00      1.00         1\n",
            "          CC       0.86      0.86      0.86         7\n",
            "          CS       0.78      1.00      0.88         7\n",
            "          DO       1.00      1.00      1.00         1\n",
            "       FW-IN       0.00      0.00      0.00         1\n",
            "       FW-NN       0.00      0.00      0.00         1\n",
            "       FW-RB       0.00      0.00      0.00         1\n",
            "          IN       0.78      0.95      0.86        19\n",
            "       IN-HL       0.00      0.00      0.00         1\n",
            "          JJ       0.71      0.83      0.77        12\n",
            "       JJ-TL       0.00      0.00      0.00         1\n",
            "         JJR       1.00      1.00      1.00         1\n",
            "          MD       1.00      1.00      1.00         1\n",
            "         MD*       1.00      1.00      1.00         2\n",
            "          NN       0.77      0.88      0.82        34\n",
            "         NN$       1.00      1.00      1.00         1\n",
            "       NN-HL       0.00      0.00      0.00         2\n",
            "         NNS       0.57      0.80      0.67         5\n",
            "          NP       1.00      0.29      0.44         7\n",
            "       NP-TL       0.00      0.00      0.00         1\n",
            "          PN       1.00      1.00      1.00         1\n",
            "         PP$       0.80      1.00      0.89         4\n",
            "        PP$$       0.00      0.00      0.00         1\n",
            "         PPO       1.00      1.00      1.00         2\n",
            "         PPS       0.50      1.00      0.67         1\n",
            "     PPS+BEZ       0.00      0.00      0.00         1\n",
            "        PPSS       1.00      1.00      1.00         7\n",
            "    PPSS+BER       1.00      0.50      0.67         2\n",
            "     PPSS+HV       0.00      0.00      0.00         1\n",
            "          QL       1.00      0.67      0.80         3\n",
            "         QLP       1.00      1.00      1.00         1\n",
            "          RB       0.67      0.40      0.50         5\n",
            "          RP       1.00      1.00      1.00         2\n",
            "          TO       0.83      1.00      0.91         5\n",
            "          VB       0.80      0.89      0.84         9\n",
            "         VBD       0.80      0.80      0.80         5\n",
            "         VBG       1.00      0.40      0.57         5\n",
            "         VBN       0.60      0.75      0.67         4\n",
            "         WDT       1.00      1.00      1.00         1\n",
            "         WP$       1.00      1.00      1.00         1\n",
            "         WPS       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.83       239\n",
            "   macro avg       0.68      0.67      0.66       239\n",
            "weighted avg       0.80      0.83      0.80       239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "corpus_concat = np.concatenate(corpus)\n",
        "\n",
        "def concat_state(s1, s2):\n",
        "    return s1 +' ' + s2\n",
        "\n",
        "states = [concat_state(corpus_concat[i][1], corpus_concat[i+1][1]) for i in range(len(corpus_concat)-1)]\n",
        "Q = list(set(states))\n",
        "len(Q)\n",
        "\n",
        "k = 0.1\n",
        "\n",
        "N, M = len(Q), len(V)\n",
        "A = np.zeros((N,N))\n",
        "B = k * np.ones((N,M))\n",
        "pi = np.zeros(N)\n",
        "\n",
        "# save indexes for each tag / word\n",
        "q_indexes, v_indexes = {}, {}\n",
        "for i, q in enumerate(Q):\n",
        "    q_indexes[q] = i\n",
        "for i, v in enumerate(V):\n",
        "    v_indexes[v] = i\n",
        "\n",
        "for sentence in training:\n",
        "    for i, s in enumerate(sentence[1:],1):\n",
        "        s_prev = sentence[i-1]\n",
        "        q = q_indexes[concat_state(s_prev[1], s[1])]\n",
        "        v = v_indexes[s[0]]    # observing second word\n",
        "        pi[q] += 1\n",
        "        B[q][v] += 1\n",
        "        if i < len(sentence)-1:\n",
        "            s_next = sentence[i+1]\n",
        "            q_next = Q.index(concat_state(s[1], s_next[1]))\n",
        "            A[q][q_next] += 1\n",
        "\n",
        "A /= np.sum(A, axis=1)[:, np.newaxis]\n",
        "B /= np.sum(B, axis=1)[:, np.newaxis]\n",
        "pi /= np.sum(pi)\n",
        "\n",
        "def beam_search(params, observations, k=2):\n",
        "    pi, A, B = params\n",
        "    M = len(observations)\n",
        "    S = pi.shape[0]\n",
        "\n",
        "    alpha = np.zeros((M, S))\n",
        "    alpha[:,:] = float('-inf')\n",
        "    backpointers = np.zeros((M, S), 'int')\n",
        "\n",
        "    alpha[0, :] = pi * B[:,observations[0]]\n",
        "\n",
        "    saved_states = np.argpartition(-alpha[0, :], k)[:k]\n",
        "\n",
        "    for t in range(1, M):\n",
        "        for s2 in range(S):\n",
        "            for s1 in saved_states:\n",
        "                score = alpha[t-1, s1] * A[s1, s2] * B[s2, observations[t]]\n",
        "                if score > alpha[t, s2]:\n",
        "                    alpha[t, s2] = score\n",
        "                    backpointers[t, s2] = s1\n",
        "        saved_states = np.argpartition(-alpha[t, :], k)[:k]\n",
        "    ss = []\n",
        "    ss.append(np.argmax(alpha[M-1,:]))\n",
        "    for i in range(M-1, 0, -1):\n",
        "        ss.append(backpointers[i, ss[-1]])\n",
        "    return list(reversed(ss)), np.max(alpha[M-1,:])\n",
        "\n",
        "\n",
        "testing_formated = []\n",
        "for sentence in testing:\n",
        "    sent = \"\"\n",
        "    words_index = []\n",
        "    true_label= []\n",
        "    for word,tag in sentence:\n",
        "        words_index.append(v_indexes[word])\n",
        "        true_label.append(tag)\n",
        "        sent=sent+\" \"+word\n",
        "    testing_formated.append((words_index,true_label,sent))\n",
        "\n",
        "\n",
        "for word_index,labels,sentence in testing_formated:\n",
        "    print(\"  The sentence is:\",sentence)\n",
        "    print(\"   ##TRUE##    ##PRED##\")\n",
        "    predicted, score = beam_search((pi, A, B), word_index, 2) #call beam search\n",
        "    for i,true_label in enumerate(labels):\n",
        "        predicted_label = Q[predicted[i]].split(\" \")[1] # decoding and taking second argument\n",
        "        print(\"      \"+true_label+\"         \"+predicted_label)\n",
        "\n",
        "true_labels, predicted_labels = [], []\n",
        "for word_index,labels,sentence in testing_formated:\n",
        "    true_labels.append(labels)\n",
        "    predicted, _ = beam_search((pi, A, B), word_index) #call the viterbi decoder\n",
        "    predicted_labels.append([Q[predicted[i]].split(\" \")[1] for i,true_label in enumerate(labels)])\n",
        "\n",
        "print(classification_report(np.concatenate(true_labels), np.concatenate(predicted_labels), zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V6bepH4jm_pf"
      },
      "source": [
        "## Using NLTK's HMM implementation\n",
        "\n",
        "We will compare now our model built from scratch to the implementation provided by NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ipL89VDm_pg",
        "outputId": "349686d1-1228-4c5e-bb5b-b9149095a52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VB'), ('up', 'IN'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', 'NP'), ('.', '.')]\n",
            "[('you', 'PPSS'), (\"can't\", 'MD*'), ('very', 'QL'), ('well', 'RB'), ('sidle', 'VBD'), ('up', 'RP'), ('to', 'IN'), ('people', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('street', 'NN'), ('and', 'CC'), ('ask', 'VB'), ('if', 'CS'), ('they', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('buy', 'VB'), ('a', 'AT'), ('hot', 'JJ'), ('Bodhisattva', '.'), ('.', '.')]\n",
            "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'FW-RB'), ('de', 'FW-IN'), ('combat', 'FW-NN'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NNS'), (',', ','), ('Delhi', 'NP'), ('boil', 'NN'), (',', ','), ('the', 'AT'), ('Granville', 'NP'), ('wilt', 'NN'), (',', ','), ('liver', 'NN'), ('fluke', 'NN'), (',', ','), ('bilharziasis', 'NN'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', 'NN'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
            "[('Additionally', 'RB'), (',', ','), ('since', 'CS'), (\"you're\", 'PPSS+BER'), ('going', 'VBG'), ('to', 'TO'), ('be', 'BE'), ('hors', 'VBN'), ('de', 'TO'), ('combat', 'VB'), ('pretty', 'QL'), ('soon', 'RB'), ('with', 'IN'), ('sprue', 'NN'), (',', ','), ('yaws', 'NP'), (',', ','), ('Delhi', 'PPSS'), ('boil', 'VB'), (',', ','), ('the', 'AT'), ('Granville', 'NP'), ('wilt', 'NP'), (',', ','), ('liver', 'NN'), ('fluke', \"''\"), (',', ','), ('bilharziasis', 'NP'), (',', ','), ('and', 'CC'), ('a', 'AT'), ('host', 'NN'), ('of', 'IN'), ('other', 'AP'), ('complications', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('hex', '``'), (\"you've\", 'PPSS+HV'), ('aroused', 'VBN'), (',', ','), ('you', 'PPSS'), (\"mustn't\", 'MD*'), ('expect', 'VB'), ('to', 'TO'), ('be', 'BE'), ('lionized', 'VBN'), ('socially', 'RB'), ('.', '.')]\n",
            "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'PPSS+BER'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
            "[('My', 'PP$'), ('advice', 'NN'), (',', ','), ('if', 'CS'), ('you', 'PPSS'), ('live', 'VB'), ('long', 'JJ'), ('enough', 'QLP'), ('to', 'TO'), ('continue', 'VB'), ('your', 'PP$'), ('vocation', 'NN'), (',', ','), ('is', 'BEZ'), ('that', 'CS'), ('the', 'AT'), ('next', 'AP'), ('time', 'NN'), (\"you're\", 'BEZ'), ('attracted', 'VBN'), ('by', 'IN'), ('the', 'AT'), ('exotic', 'JJ'), (',', ','), ('pass', 'VB'), ('it', 'PPO'), ('up', 'RP'), ('--', '--'), (\"it's\", 'PPS+BEZ'), ('nothing', 'PN'), ('but', 'CC'), ('a', 'AT'), ('headache', 'NN'), ('.', '.')]\n",
            "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
            "[('As', 'CS'), ('you', 'PPSS'), ('can', 'MD'), ('count', 'VB'), ('on', 'IN'), ('me', 'PPO'), ('to', 'TO'), ('do', 'DO'), ('the', 'AT'), ('same', 'AP'), ('.', '.')]\n",
            "[('Compassionately', 'RB'), ('yours', 'PP$$'), (',', ',')]\n",
            "[('Compassionately', '``'), ('yours', 'UH'), (',', ',')]\n",
            "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
            "[('S.', 'NP'), ('J.', 'NP'), ('Perelman', 'NP')]\n",
            "[('revulsion', 'NN-HL'), ('in', 'IN-HL'), ('the', 'AT-HL'), ('desert', 'NN-HL')]\n",
            "[('revulsion', 'NN'), ('in', 'IN'), ('the', 'AT'), ('desert', 'NN')]\n",
            "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NP-TL'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'VBG'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NN'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'VBD'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
            "[('the', 'AT'), ('doors', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('D', 'NN'), ('train', 'NN'), ('slid', 'VBD'), ('shut', 'VBN'), (',', ','), ('and', 'CC'), ('as', 'CS'), ('I', 'PPSS'), ('dropped', 'VBD'), ('into', 'IN'), ('a', 'AT'), ('seat', 'NN'), ('and', 'CC'), (',', ','), ('exhaling', 'NP'), (',', ','), ('looked', 'VBD'), ('up', 'RP'), ('across', 'IN'), ('the', 'AT'), ('aisle', 'NN'), (',', ','), ('the', 'AT'), ('whole', 'JJ'), ('aviary', 'NNS'), ('in', 'IN'), ('my', 'PP$'), ('head', 'NN'), ('burst', 'NN'), ('into', 'IN'), ('song', 'NN'), ('.', '.')]\n",
            "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'JJ'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'NP'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ-TL'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'VBG'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
            "[('She', 'PPS'), ('was', 'BEDZ'), ('a', 'AT'), ('living', 'VBG'), ('doll', 'NN'), ('and', 'CC'), ('no', 'AT'), ('mistake', 'NN'), ('--', '--'), ('the', 'AT'), ('blue-black', 'JJ'), ('bang', 'NN'), (',', ','), ('the', 'AT'), ('wide', 'JJ'), ('cheekbones', 'NNS'), (',', ','), ('olive-flushed', 'NP'), (',', ','), ('that', 'WPS'), ('betrayed', 'VBD'), ('the', 'AT'), ('Cherokee', 'JJ'), ('strain', 'NN'), ('in', 'IN'), ('her', 'PP$'), ('Midwestern', 'JJ'), ('lineage', 'NN'), (',', ','), ('and', 'CC'), ('the', 'AT'), ('mouth', 'NN'), ('whose', 'WP$'), ('only', 'AP'), ('fault', 'NN'), (',', ','), ('in', 'IN'), ('the', 'AT'), (\"novelist's\", 'NN$'), ('carping', 'NN'), ('phrase', 'NN'), (',', ','), ('was', 'BEDZ'), ('that', 'CS'), ('the', 'AT'), ('lower', 'JJR'), ('lip', 'NN'), ('was', 'BEDZ'), ('a', 'AT'), ('trifle', 'NN'), ('too', 'QL'), ('voluptuous', 'JJ'), ('.', '.')]\n",
            "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'JJ'), ('boucle', 'NN'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBG'), ('.', '.')]\n",
            "[('From', 'IN'), ('what', 'WDT'), ('I', 'PPSS'), ('was', 'BEDZ'), ('able', 'JJ'), ('to', 'IN'), ('gauge', 'NN'), ('in', 'IN'), ('a', 'AT'), ('swift', 'JJ'), (',', ','), ('greedy', 'JJ'), ('glance', 'NN'), (',', ','), ('the', 'AT'), ('figure', 'NN'), ('inside', 'IN'), ('the', 'AT'), ('coral-colored', 'QL'), ('boucle', 'JJ'), ('dress', 'NN'), ('was', 'BEDZ'), ('stupefying', 'VBN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer(states = Q, symbols = V)\n",
        "\n",
        "model = trainer.train_supervised(training, estimator=lambda fd, bins: hmm.LidstoneProbDist(fd, 0.1, bins))\n",
        "\n",
        "for sent in testing:\n",
        "    u_sent=[]\n",
        "    for word, tag in sent:\n",
        "        u_sent.append(word)\n",
        "    tagged=model.tag(u_sent)\n",
        "    print(sent)\n",
        "    print(tagged)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7zKO2sBHm_pg"
      },
      "source": [
        "**Exercise 5**: Calculate precision, recall and F-measure and compare them to the results that you obtained with the two models (bigram and trigram) that you implemented before. Can you deduce whether the NLTK model is using bigrams or trigrams? (It is not stated in the manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0b1iuiWZ_VQI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ''       0.00      0.00      0.00         0\n",
            "           ,       1.00      1.00      1.00        24\n",
            "          --       1.00      1.00      1.00         2\n",
            "           .       0.88      1.00      0.93         7\n",
            "          AP       1.00      1.00      1.00         4\n",
            "          AT       0.96      1.00      0.98        26\n",
            "       AT-HL       0.00      0.00      0.00         1\n",
            "          BE       1.00      1.00      1.00         2\n",
            "        BEDZ       1.00      1.00      1.00         5\n",
            "         BEZ       0.50      1.00      0.67         1\n",
            "          CC       1.00      1.00      1.00         7\n",
            "          CS       1.00      1.00      1.00         7\n",
            "          DO       1.00      1.00      1.00         1\n",
            "       FW-IN       0.00      0.00      0.00         1\n",
            "       FW-NN       0.00      0.00      0.00         1\n",
            "       FW-RB       0.00      0.00      0.00         1\n",
            "          IN       0.95      0.95      0.95        19\n",
            "       IN-HL       0.00      0.00      0.00         1\n",
            "          JJ       0.77      0.83      0.80        12\n",
            "       JJ-TL       0.00      0.00      0.00         1\n",
            "         JJR       1.00      1.00      1.00         1\n",
            "          MD       1.00      1.00      1.00         1\n",
            "         MD*       1.00      1.00      1.00         2\n",
            "          NN       0.84      0.79      0.82        34\n",
            "         NN$       1.00      1.00      1.00         1\n",
            "       NN-HL       0.00      0.00      0.00         2\n",
            "         NNS       0.80      0.80      0.80         5\n",
            "          NP       0.44      0.57      0.50         7\n",
            "       NP-TL       0.00      0.00      0.00         1\n",
            "          PN       1.00      1.00      1.00         1\n",
            "         PP$       1.00      1.00      1.00         4\n",
            "        PP$$       0.00      0.00      0.00         1\n",
            "         PPO       1.00      1.00      1.00         2\n",
            "         PPS       1.00      1.00      1.00         1\n",
            "     PPS+BEZ       1.00      1.00      1.00         1\n",
            "        PPSS       0.88      1.00      0.93         7\n",
            "    PPSS+BER       1.00      0.50      0.67         2\n",
            "     PPSS+HV       1.00      1.00      1.00         1\n",
            "          QL       0.75      1.00      0.86         3\n",
            "         QLP       1.00      1.00      1.00         1\n",
            "          RB       1.00      0.80      0.89         5\n",
            "          RP       0.67      1.00      0.80         2\n",
            "          TO       0.83      1.00      0.91         5\n",
            "          UH       0.00      0.00      0.00         0\n",
            "          VB       0.80      0.89      0.84         9\n",
            "         VBD       0.80      0.80      0.80         5\n",
            "         VBG       1.00      0.40      0.57         5\n",
            "         VBN       0.67      1.00      0.80         4\n",
            "         WDT       1.00      1.00      1.00         1\n",
            "         WP$       1.00      1.00      1.00         1\n",
            "         WPS       1.00      1.00      1.00         1\n",
            "          ``       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87       239\n",
            "   macro avg       0.70      0.72      0.70       239\n",
            "weighted avg       0.86      0.87      0.86       239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred, y_true = [], []\n",
        "\n",
        "for sent in testing:\n",
        "    u_sent=[]\n",
        "    for word, tag in sent:\n",
        "        u_sent.append(word)\n",
        "    tagged = model.tag(u_sent)\n",
        "\n",
        "    y_pred.extend([tup[1] for tup in tagged])\n",
        "    y_true.extend([tup[1] for tup in sent])\n",
        "\n",
        "print(classification_report(y_true=y_true, y_pred=y_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "beWrHG8Dm_pg",
        "outputId": "bfbf28db-8bc7-4916-e92a-7ece0dd0627c"
      },
      "outputs": [],
      "source": [
        "#yp, yt = [], []\n",
        "\n",
        "#for s in testing:\n",
        " #   u = []\n",
        " #   for w, tag in s:\n",
        " #       u_sent.append(w)\n",
        "#    tagged = model.tag(u)\n",
        "  #\n",
        "   # yp.extend([tup[1] for tup in tagged])\n",
        "  #  yt.extend([tup[1] for tup in s])\n",
        "\n",
        "#print(classification_report(y_true=yt, y_pred=yp, zero_division=0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yL03_xDCm_ph"
      },
      "source": [
        "## Named Entity Recognition with Conditional Random Fields\n",
        "\n",
        "For this exercise we will need to use the sklearn_crfsuite package. If it is not installed, it can be installed using pip with ```pip install sklearn-crfsuite```.\n",
        "\n",
        "We will work on a Kaggle dataset named ```ner_dataset.csv``` (it should be in the same directory as the notebook).\n",
        "\n",
        "Pandas can be used to read the content of the file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ELikiu2om_ph"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1356789/4147749998.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['non-cooperation', 'Ibobi', 'militancy', 'Davydenko', 'Sperling', 'Karshi-Khanabad', 'Syncardia', 'tolerate', 'hair', 'Financial']\n",
            "35177\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
        "data = data.fillna(method=\"ffill\") #repeat sentence number on each row\n",
        "\n",
        "words = list(set(data[\"Word\"].values)) #vocabulary V\n",
        "n_words = len(words)\n",
        "\n",
        "print(words[:10])\n",
        "print(n_words)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oSDhyIaBm_ph"
      },
      "source": [
        "We provide you with some code that can read the sentences and produce the features in the format required by crf_suite. The ```SentenceGetter``` class transforms sentences into sequences of ```(word, POS, tag)``` triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JeuZ4t70m_ph"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1347756/985691969.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n"
          ]
        }
      ],
      "source": [
        "class SentenceGetter(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"POS\"].values.tolist(),\n",
        "                                                           s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "#load data\n",
        "getter = SentenceGetter(data) #transform sentences into sequences of (Word, POS, Tag)\n",
        "sentences = getter.sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ccZEqeEtm_ph"
      },
      "source": [
        "The next function allows us to define features that are used in the CRF. The features are stored in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uE9Tvkrmm_pi"
      },
      "outputs": [],
      "source": [
        "def word2features(sent, i):\n",
        "    \"\"\"\n",
        "    input:\n",
        "       sent: sentence in the format of sequence of (Word, POS, Tag) triples\n",
        "       i: position in the sentence\n",
        "    output:\n",
        "       features: a dictionary mapping the feature name into a value\n",
        "    \"\"\"\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = { #features related to the current position\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'postag': postag,\n",
        "    }\n",
        "    if i > 0: #features related to preceding word/tag\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:postag': postag1,\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True #feature for Beginning of Sentence\n",
        "\n",
        "    if i < len(sent)-1: #features related to the following word/tag\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:postag': postag1,\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True #feature for end of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    #transforms the sentence in a sequence of features\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    #transforms the sentence in a sequence of labels\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    #transforms the sentence in a sequence of tokens (removes POS tags and labels)\n",
        "    return [token for token, postag, label in sent]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KFEI1TXkm_pi"
      },
      "source": [
        "We can now build the features and label vectors, and create a CRF model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m7j8nv3Hm_pj"
      },
      "outputs": [],
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "\n",
        "from sklearn_crfsuite import CRF\n",
        "crf = CRF(algorithm='lbfgs',  max_iterations=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "45DHNccIm_pj"
      },
      "source": [
        "This will create a model with gradient descent algorithm (\"lbfgs\") and a limit of $100$ iterations.\n",
        "\n",
        "Now we build the model and evaluate it on a 66/33 split between training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PNpUwyS3m_pj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.40      0.01      0.03       137\n",
            "       B-eve       0.69      0.31      0.42       111\n",
            "       B-geo       0.83      0.91      0.87     12357\n",
            "       B-gpe       0.98      0.82      0.89      5226\n",
            "       B-nat       0.60      0.09      0.15        69\n",
            "       B-org       0.78      0.67      0.72      6762\n",
            "       B-per       0.83      0.79      0.81      5649\n",
            "       B-tim       0.93      0.84      0.88      6650\n",
            "       I-art       0.38      0.02      0.05       124\n",
            "       I-eve       0.56      0.21      0.31        89\n",
            "       I-geo       0.80      0.79      0.80      2433\n",
            "       I-gpe       0.95      0.33      0.49        55\n",
            "       I-nat       0.33      0.05      0.08        21\n",
            "       I-org       0.76      0.78      0.77      5545\n",
            "       I-per       0.85      0.87      0.86      5730\n",
            "       I-tim       0.81      0.74      0.78      2110\n",
            "           O       0.99      0.99      0.99    292571\n",
            "\n",
            "    accuracy                           0.97    345639\n",
            "   macro avg       0.73      0.54      0.58    345639\n",
            "weighted avg       0.97      0.97      0.96    345639\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "crf.fit(X_train, y_train)\n",
        "pred=crf.predict(X_test)\n",
        "\n",
        "n_pred = [item for sublist in pred for item in sublist]\n",
        "n_test = [item for sublist in y_test for item in sublist]\n",
        "\n",
        "report = classification_report(y_pred=n_pred, y_true=n_test)\n",
        "print(report)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yELnmk3fm_pj"
      },
      "source": [
        "The report shows accuracy stats for all classes, but we are not interested in the **O** class. We can see that the scores for people names, place names and organizations are relatively low.\n",
        "\n",
        "**Exercise 6**: Can you think of some new features for the CRF model to improve the results, especially on **B-org** ? Modify the *word2features* function to include the additional features and compare with the above results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "di4pLr-l6ah2"
      },
      "outputs": [],
      "source": [
        "from sklearn_crfsuite import CRF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.24      0.03      0.05       137\n",
            "       B-eve       0.65      0.32      0.42       111\n",
            "       B-geo       0.83      0.91      0.87     12357\n",
            "       B-gpe       0.95      0.90      0.93      5226\n",
            "       B-nat       0.67      0.03      0.06        69\n",
            "       B-org       0.79      0.69      0.74      6762\n",
            "       B-per       0.83      0.79      0.81      5649\n",
            "       B-tim       0.93      0.84      0.88      6650\n",
            "       I-art       0.31      0.04      0.07       124\n",
            "       I-eve       0.50      0.21      0.30        89\n",
            "       I-geo       0.81      0.78      0.79      2433\n",
            "       I-gpe       0.96      0.45      0.62        55\n",
            "       I-nat       0.00      0.00      0.00        21\n",
            "       I-org       0.77      0.80      0.78      5545\n",
            "       I-per       0.83      0.91      0.87      5730\n",
            "       I-tim       0.81      0.74      0.77      2110\n",
            "           O       0.99      0.99      0.99    292571\n",
            "\n",
            "    accuracy                           0.97    345639\n",
            "   macro avg       0.70      0.55      0.59    345639\n",
            "weighted avg       0.97      0.97      0.97    345639\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def word2features_new(sent, i):\n",
        "\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = { \n",
        "        'bias': 1.0,\n",
        "        'upper_first': int(word[0].isupper()),\n",
        "        'upper_whole': int(word.isupper()),\n",
        "        'word.lower()': word.lower(),\n",
        "        'postag': postag,\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:postag': postag1,\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True \n",
        "    if i < len(sent)-1: \n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:postag': postag1,\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True \n",
        "    return features\n",
        "\n",
        "def sent2features_new(sent):\n",
        "    return [word2features_new(sent, i) for i in range(len(sent))]\n",
        "\n",
        "X = [sent2features_new(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]\n",
        "\n",
        "from sklearn_crfsuite import CRF\n",
        "crf = CRF(algorithm='lbfgs',  max_iterations=100)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "crf.fit(X_train, y_train)\n",
        "pred=crf.predict(X_test)\n",
        "\n",
        "pred_flat = [item for sublist in pred for item in sublist]\n",
        "true_flat = [item for sublist in y_test for item in sublist]\n",
        "\n",
        "report = classification_report(y_pred=pred_flat, y_true=true_flat, zero_division=0)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lz7KPNgAm_pj"
      },
      "outputs": [],
      "source": [
        "# def word2features_new(sent, i):\n",
        "#     word = sent[i][0]\n",
        "#     postag = sent[i][1]\n",
        "\n",
        "#     features = {\n",
        "#         'bias': 1.0,\n",
        "#         'upper_first': int(word[0].isupper()),\n",
        "#         'upper_whole': int(word.isupper()),\n",
        "#         'word.lower()': word.lower(),\n",
        "#         'postag': postag,\n",
        "#     }\n",
        "#     if i > 0:\n",
        "#         word1 = sent[i-1][0]\n",
        "#         postag1 = sent[i-1][1]\n",
        "#         features.update({\n",
        "#             '-1:word.lower()': word1.lower(),\n",
        "#             '-1:postag': postag1,\n",
        "#         })\n",
        "#     else:\n",
        "#         features['BOS'] = True\n",
        "#     if i < len(sent)-1:\n",
        "#         word1 = sent[i+1][0]\n",
        "#         postag1 = sent[i+1][1]\n",
        "#         features.update({\n",
        "#             '+1:word.lower()': word1.lower(),\n",
        "#             '+1:postag': postag1,\n",
        "#         })\n",
        "#     else:\n",
        "#         features['EOS'] = True\n",
        "#     return features\n",
        "\n",
        "# def sent2features_new(sent):\n",
        "#     return [word2features_new(sent, i) for i in range(len(sent))]\n",
        "\n",
        "# X = [sent2features_new(s) for s in sentences]\n",
        "# y = [sent2labels(s) for s in sentences]\n",
        "\n",
        "# crf = CRF(algorithm='lbfgs',  max_iterations=100)\n",
        "\n",
        "# Xt, Xtest, yt, ytest = train_test_split(X, y, test_size=0.3)\n",
        "# crf.fit(Xt, yt)\n",
        "# p=crf.predict(Xtest)\n",
        "\n",
        "# pflat = [item for sublist in p for item in sublist]\n",
        "# tflat = [item for sublist in y_test for item in sublist]\n",
        "\n",
        "# report = classification_report(y_pred=pflat, y_true=tflat, zero_division=0)\n",
        "# print(report)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Blz0y-rqm_pj"
      },
      "source": [
        "## NER using a LSTM model\n",
        "\n",
        "In this final section we will see an example of a neural network model written in PyTorch that uses a LSTM-based architecture for Named Entity Recognition.\n",
        "\n",
        "First of all, we will prepare the data to have all information coded numerically (words and tags) and the sentences padded to a max length, in order to have all sentences of the same size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fF08nDXdm_pk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-07 20:18:56.153471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-07 20:18:56.153496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-07 20:18:56.154194: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-07 20:18:56.158183: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-07 20:18:57.280391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "#first of all we need to retrieve the number of different tags (a.k.a categories or classes of the words)\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "n_tags=len(tags)\n",
        "\n",
        "vocab = {w: i + 1 for i, w in enumerate(words)} #map words into a number\n",
        "tag_map = {t: i for i, t in enumerate(tags)} #map tags into a number\n",
        "\n",
        "max_len=75\n",
        "from tensorflow import keras \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = [[vocab[w[0]] for w in s] for s in sentences]\n",
        "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words) #pad with special token PAD, with ID=n_words\n",
        "y = [[tag_map[w[2]] for w in s] for s in sentences]\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=-1) # -1 is associated to the PAD token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: keras in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (2.15.0.post1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/lib/python3.9/site-packages (from tensorflow) (20.9)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from tensorflow) (53.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.9/site-packages (from packaging->tensorflow) (2.4.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (7.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /users/eleves-b/2021/joao.sedeu-godoi/.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install keras tensorflow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVKRBaNm_pk"
      },
      "source": [
        "Now we will load the data and split them into training and test. We set batch size at 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPBzp_J3m_pk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mNERDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m NERDataset\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.33\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m ner_train \u001b[39m=\u001b[39m NERDataset(X_train, y_train)\n\u001b[1;32m      8\u001b[0m ner_test \u001b[39m=\u001b[39m NERDataset(X_test, y_test)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from NERDataset import NERDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "ner_train = NERDataset(X_train, y_train)\n",
        "ner_test = NERDataset(X_test, y_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(ner_train, batch_size=32, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yQfNP-b-m_pk"
      },
      "source": [
        "The LSTM model is defined here. We have an embedding that maps each word in a vector (embedding) of size 100, which is learnt from the dataset. The embedded sentence is fed to a LSTM layer of size 50. The output is transferred to a fully connected layer with *n_tags* output, one for each of the possible labels. The loss is a cross-entropy loss over all tokens (excluding the \"pad\" tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t32tY7I8m_pl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "vocab_size=n_words+1\n",
        "embedding_dim=100\n",
        "lstm_hidden_dim=50\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        #maps each token to an embedding_dim vector\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #the LSTM takens embedded sentence\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        #fc layer transforms the output to give the final output layer\n",
        "        self.fc = nn.Linear(lstm_hidden_dim, n_tags)\n",
        "\n",
        "    def forward(self, s):\n",
        "        #apply the embedding layer that maps each token to its embedding\n",
        "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
        "\n",
        "        #run the LSTM along the sentences of length batch_max_len. We discard the cell state as output\n",
        "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim\n",
        "\n",
        "        #reshape the Variable so that each row contains one token\n",
        "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
        "\n",
        "        #apply the fully connected layer and obtain the output for each token\n",
        "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
        "\n",
        "        return F.log_softmax(s, dim=1)   # dim: batch_size*batch_max_len x num_tags\n",
        "\n",
        "def loss_fn(outputs, labels):\n",
        "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
        "    labels = labels.view(-1)\n",
        "\n",
        "    #discard 'PAD' tokens\n",
        "    mask = (labels >= 0).float()\n",
        "\n",
        "    #the number of tokens is the sum of elements in mask\n",
        "    num_tokens = int(torch.sum(mask))\n",
        "\n",
        "    #pick the values corresponding to labels and multiply by mask\n",
        "    outputs = outputs[range(outputs.shape[0]), labels]*mask\n",
        "\n",
        "    #cross entropy loss for all non 'PAD' tokens\n",
        "    return -torch.sum(outputs)/num_tokens\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSR_09UWm_pl"
      },
      "source": [
        "In the following block we carry out the training over 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hyD6tJSZm_pl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 1.583\n",
            "Loss after mini-batch  1000: 0.830\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 0.743\n",
            "Loss after mini-batch  1000: 0.676\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 0.615\n",
            "Loss after mini-batch  1000: 0.569\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 0.520\n",
            "Loss after mini-batch  1000: 0.482\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 0.447\n",
            "Loss after mini-batch  1000: 0.415\n"
          ]
        }
      ],
      "source": [
        "network = Net()\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs=5\n",
        "#Run the training loop for defined number of epochs\n",
        "for epoch in range(0, num_epochs):\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    i=0\n",
        "    for inputs, targets in trainloader:\n",
        "        #print(inputs, targets)\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        outputs = network(inputs) # Perform forward pass\n",
        "        loss = loss_fn(outputs, targets) # Compute loss\n",
        "        loss.backward() # Backprop\n",
        "        optimizer.step() # Optimization\n",
        "\n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 500 == 499:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 500))\n",
        "            current_loss = 0.0\n",
        "        i+=1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv7RUPOxm_pm"
      },
      "source": [
        "**Exercise 7**: Evaluate the model over the test set (see the data preparation cells), compare the result with the previously seen CRF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-art       0.00      0.00      0.00       137\n",
            "       B-eve       0.00      0.00      0.00       111\n",
            "       B-geo       0.59      0.62      0.60     12357\n",
            "       B-gpe       0.92      0.30      0.45      5226\n",
            "       B-nat       0.00      0.00      0.00        69\n",
            "       B-org       0.68      0.03      0.05      6762\n",
            "       B-per       0.82      0.40      0.54      5649\n",
            "       B-tim       0.82      0.48      0.61      6650\n",
            "       I-art       0.00      0.00      0.00       124\n",
            "       I-eve       0.00      0.00      0.00        89\n",
            "       I-geo       0.91      0.16      0.28      2433\n",
            "       I-gpe       0.00      0.00      0.00        55\n",
            "       I-nat       0.00      0.00      0.00        21\n",
            "       I-org       0.65      0.29      0.40      5545\n",
            "       I-per       0.65      0.58      0.61      5730\n",
            "       I-tim       0.00      0.00      0.00      2110\n",
            "           O       0.92      0.99      0.95    292571\n",
            "\n",
            "    accuracy                           0.90    345639\n",
            "   macro avg       0.41      0.23      0.26    345639\n",
            "weighted avg       0.88      0.90      0.88    345639\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=False)\n",
        "\n",
        "yp, yt = [], []\n",
        "\n",
        "for inp, targ in test:\n",
        "        network.eval()\n",
        "        outputs = network(inp)\n",
        "        pred = torch.argmax(outputs.view(-1,75,17), dim=2)\n",
        "        \n",
        "        pf, tf = pred.flatten(), targ.flatten()\n",
        "        for i in range(len(pred.flatten())):\n",
        "            if tf[i] != -1:\n",
        "                yp.append(tags[pf[i]])\n",
        "                yt.append(tags[tf[i]])\n",
        "                \n",
        "r = classification_report(y_pred=yp, y_true=yt, zero_division=0)\n",
        "print(r)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RFPuDQHam_pm"
      },
      "source": [
        "**Exercise 8**: Modify the model to use GloVe vectors as initial embeddings (hint: see the documentation of the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\">Embedding</a> layer in PyTorch, in particular the *from_pretrained* method), and compare the results to the base one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5PTAOKTam_pm"
      },
      "outputs": [
        {
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m GloVe\n\u001b[0;32m----> 2\u001b[0m glove \u001b[39m=\u001b[39m GloVe(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m6B\u001b[39;49m\u001b[39m\"\u001b[39;49m, dim\u001b[39m=\u001b[39;49membedding_dim, max_vectors\u001b[39m=\u001b[39;49mvocab_size)\n\u001b[1;32m      4\u001b[0m network \u001b[39m=\u001b[39m Net()\n\u001b[1;32m      5\u001b[0m network\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding\u001b[39m.\u001b[39mfrom_pretrained(glove\u001b[39m.\u001b[39mvectors)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:223\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[1;32m    222\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[0;32m--> 223\u001b[0m \u001b[39msuper\u001b[39;49m(GloVe, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[0;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:105\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    103\u001b[0m ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(dest)[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(dest, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m zf:\n\u001b[1;32m    106\u001b[0m         zf\u001b[39m.\u001b[39mextractall(cache)\n\u001b[1;32m    107\u001b[0m \u001b[39melif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgz\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m/usr/lib64/python3.9/zipfile.py:1266\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m-> 1266\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RealGetContents()\n\u001b[1;32m   1267\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1268\u001b[0m         \u001b[39m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m         \u001b[39m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_didModify \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib64/python3.9/zipfile.py:1333\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1334\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1335\u001b[0m     \u001b[39mprint\u001b[39m(endrec)\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import GloVe\n",
        "glove = GloVe(name=\"6B\", dim=embedding_dim, max_vectors=vocab_size)\n",
        "\n",
        "network = Net()\n",
        "network.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
        "\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs=5\n",
        "for epoch in range(0, num_epochs):\n",
        "    print(f'epoch {epoch+1}')\n",
        "    losst = 0.0\n",
        "    i=0\n",
        "    for inp, targ in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = network(inp)\n",
        "        loss = loss_fn(outputs, targ)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losst += loss.item()\n",
        "        if i % 500 == 499:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, losst / 500))\n",
        "            losst = 0.0\n",
        "        i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMbqkBQ94J5u"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 1425 is out of bounds for dimension 0 with size 1425",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m pflat, tflat \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mflatten(), targets\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(pred\u001b[39m.\u001b[39mflatten())):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mif\u001b[39;00m tflat[i] \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m         yp\u001b[39m.\u001b[39mappend(tags[pflat[i]])\n\u001b[1;32m     13\u001b[0m         yt\u001b[39m.\u001b[39mappend(tags[tflat[i]])\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1425 is out of bounds for dimension 0 with size 1425"
          ]
        }
      ],
      "source": [
        "testloader = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=False)\n",
        "yp, yt = [], []\n",
        "\n",
        "for inp, targ in testloader:\n",
        "        network.eval()\n",
        "        outputs = network(inp)\n",
        "        pred = torch.argmax(outputs.view(-1,75,17), dim=2)\n",
        "\n",
        "        pflat, tflat = pred.flatten(), targets.flatten()\n",
        "        for i in range(len(pred.flatten())):\n",
        "            if tflat[i] != -1:\n",
        "                yp.append(tags[pflat[i]])\n",
        "                yt.append(tags[tflat[i]])\n",
        "\n",
        "r = classification_report(y_pred=yp, y_true=yt, zero_division=0)\n",
        "print(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "BadZipFile",
          "evalue": "File is not a zip file",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m GloVe\n\u001b[0;32m----> 2\u001b[0m glove \u001b[39m=\u001b[39m GloVe(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m6B\u001b[39;49m\u001b[39m\"\u001b[39;49m, dim \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, max_vectors\u001b[39m=\u001b[39;49mvocab_size)\n\u001b[1;32m      4\u001b[0m network \u001b[39m=\u001b[39m Net()\n\u001b[1;32m      5\u001b[0m network\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding\u001b[39m.\u001b[39mfrom_pretrained(glove\u001b[39m.\u001b[39mvectors)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:223\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[1;32m    222\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[0;32m--> 223\u001b[0m \u001b[39msuper\u001b[39;49m(GloVe, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[0;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/vocab/vectors.py:105\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    103\u001b[0m ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(dest)[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39;49mZipFile(dest, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m zf:\n\u001b[1;32m    106\u001b[0m         zf\u001b[39m.\u001b[39mextractall(cache)\n\u001b[1;32m    107\u001b[0m \u001b[39melif\u001b[39;00m ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgz\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[0;32m/usr/lib64/python3.9/zipfile.py:1266\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m-> 1266\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RealGetContents()\n\u001b[1;32m   1267\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1268\u001b[0m         \u001b[39m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m         \u001b[39m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_didModify \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib64/python3.9/zipfile.py:1333\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endrec:\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mraise\u001b[39;00m BadZipFile(\u001b[39m\"\u001b[39m\u001b[39mFile is not a zip file\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1334\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1335\u001b[0m     \u001b[39mprint\u001b[39m(endrec)\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import GloVe\n",
        "glove = GloVe(name=\"6B\", dim = 100, max_vectors=vocab_size)\n",
        "\n",
        "network = Net()\n",
        "network.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs=5\n",
        "#Run the training loop for defined number of epochs\n",
        "for epoch in range(0, num_epochs):\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Set current loss value\n",
        "    current_loss = 0.0\n",
        "    \n",
        "    i=0\n",
        "    for inputs, targets in trainloader:\n",
        "        #print(inputs, targets)\n",
        "        \n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        outputs = network(inputs) # Perform forward pass\n",
        "        loss = loss_fn(outputs, targets) # Compute loss\n",
        "        loss.backward() # Backprop\n",
        "        optimizer.step() # Optimization\n",
        "        \n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        if i % 500 == 499:\n",
        "            print('Loss after mini-batch %5d: %.3f' %\n",
        "                  (i + 1, current_loss / 500))\n",
        "            current_loss = 0.0\n",
        "        i+=1\n",
        "\n",
        "\n",
        "# Now re-evaluate\n",
        "testloader = torch.utils.data.DataLoader(ner_test, batch_size=32, shuffle=False)\n",
        "\n",
        "y_pred, y_true = [], []\n",
        "\n",
        "for inputs, targets in testloader:\n",
        "        network.eval()\n",
        "        outputs = network(inputs)\n",
        "        pred = torch.argmax(outputs.view(-1,75,17), dim=2)\n",
        "        \n",
        "        pred_flat, target_flat = pred.flatten(), targets.flatten()\n",
        "        for i in range(len(pred.flatten())):\n",
        "            if target_flat[i] != -1:\n",
        "                y_pred.append(tags[pred_flat[i]])\n",
        "                y_true.append(tags[target_flat[i]])\n",
        "                \n",
        "report = classification_report(y_pred=y_pred, y_true=y_true, zero_division=0)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
